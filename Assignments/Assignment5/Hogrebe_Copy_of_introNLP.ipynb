{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Adapted from Deep Learning with Python, 2e and 3e, Chollet and Chollet and Watson."
      ],
      "metadata": {
        "id": "Yom3Z8O_c_Bi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Block 0A:  Import general purpose packages for ease of use and improved performance."
      ],
      "metadata": {
        "id": "SxMReo5oq986"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import collections\n",
        "import re\n",
        "import string"
      ],
      "metadata": {
        "id": "HcrDIqY4q3J7"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Block 0B:  Import packages for a typical deep learning workflow using TensorFlow and Keras.  Import the IMDB dataset from Keras."
      ],
      "metadata": {
        "id": "VVU9OI61NZkk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "YHxwv0dzNLVD"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Block 1A:  Define a function to create a vocabulary using single characters as tokens."
      ],
      "metadata": {
        "id": "wALmViBcFxGi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def set_charlvl_vocabulary(inputs, max_size):\n",
        "    # Determine the number of occurrences of each character-level\n",
        "    # token in the text sample.\n",
        "    char_counts = collections.Counter()\n",
        "    for elem in inputs:\n",
        "        elem = elem.lower()\n",
        "        tokens = re.findall(r'.', elem)\n",
        "        char_counts.update(tokens)\n",
        "\n",
        "    # Initialize the vocabulary with the tokens used to pad vectorized\n",
        "    # text to a consistent length and to signal an out-of-vocabulary\n",
        "    # word.\n",
        "    vocabulary = ['[PAD]', '[UNK]']\n",
        "\n",
        "    # Fill the remainder of the vocabulary with the most common tokens.\n",
        "    most_common = char_counts.most_common(max_size - len(vocabulary))\n",
        "    for token, count in most_common:\n",
        "        vocabulary.append(token)\n",
        "\n",
        "    # This function returns a Python **dict** mapping each character token\n",
        "    # (string) to its integer index. Verified by calling:\n",
        "    #     >>> type(set_charlvl_vocabulary([\"Hello!\"], 50))\n",
        "    #     <class 'dict'>\n",
        "    return {token: i for i, token in enumerate(vocabulary)}\n"
      ],
      "metadata": {
        "id": "RuxuVU6oF3G0"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The **encoded length** (X) roughly matches the count of words/characters in our test sentence.\n",
        "- Any discrepancies arise because words not in the top-N get mapped to `[UNK]`, and padding or truncation may occur.\n"
      ],
      "metadata": {
        "id": "xfVqL7aXFrZB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Block 1B:  Define a function that creates a vocabulary using individual words as tokens."
      ],
      "metadata": {
        "id": "W53BkMDebVn5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def set_wordlvl_vocabulary(inputs, max_size):\n",
        "    # Determine the number of occurrences of each word-level\n",
        "    # token in the text sample.\n",
        "    word_counts = collections.Counter()\n",
        "    for elem in inputs:\n",
        "        elem = elem.lower()\n",
        "        # Regex explanation:\n",
        "        #   [\\w]+    → one or more “word” characters (letters, digits, underscore)\n",
        "        #   [.,!?;]? → optionally followed by a single punctuation mark\n",
        "        tokens = re.findall(r'[\\w]+[.,!?;]?', elem)\n",
        "        word_counts.update(tokens)\n",
        "\n",
        "    # Initialize the vocabulary with the tokens used to pad vectorized\n",
        "    # text to a consistent length and to signal an out-of-vocabulary word.\n",
        "    vocabulary = ['[PAD]', '[UNK]']\n",
        "\n",
        "    # Fill the remainder of the vocabulary with the most common tokens.\n",
        "    most_common = word_counts.most_common(max_size - len(vocabulary))\n",
        "    for token, count in most_common:\n",
        "        vocabulary.append(token)\n",
        "\n",
        "    # This function returns a Python **dict** mapping each word token\n",
        "    # (a string like \"hello\" or \"test,\") to a unique integer index.\n",
        "    # We verified this by calling:\n",
        "    #     >>> type(set_wordlvl_vocabulary([\"This is a test.\"], 50))\n",
        "    #     <class 'dict'>\n",
        "    return {token: i for i, token in enumerate(vocabulary)}"
      ],
      "metadata": {
        "id": "Kv_HGpM_bX-2"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Regex difference in 1A vs. 1B**\n",
        "\n",
        "- **Char-level** uses `r'.'` to grab every single character (letters, spaces, punctuation).\n",
        "- **Word-level** uses `r'[\\w]+[.,!?;]?` to grab whole words plus an optional punctuation mark.\n"
      ],
      "metadata": {
        "id": "CN3svR1y23OH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Block 2A:  Define a class for pre-processing text data at the character level."
      ],
      "metadata": {
        "id": "ocxE4EK0XsVW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "QA6ELlSPS-wc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6edf539e-bd7f-4472-9c7c-d67cd7e2e59f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: Ab a!\n",
            "Encoded IDs: [2, 3, 4, 2]\n",
            "Decoded back: ab a\n",
            "encode() returns: <class 'list'> with length 4\n"
          ]
        }
      ],
      "source": [
        "class CharTokenizer:\n",
        "    def __init__(self, vocabulary: dict[str, int]):\n",
        "        \"\"\"\n",
        "        Initialize a character-level tokenizer.\n",
        "\n",
        "        Args:\n",
        "            vocabulary: A dict mapping character tokens (str) to integer indices.\n",
        "                        Must include '[PAD]' and '[UNK]'.\n",
        "        \"\"\"\n",
        "        self.vocabulary: dict[str, int] = vocabulary\n",
        "        self.pad_id: int = vocabulary['[PAD]']\n",
        "        self.unk_id: int = vocabulary['[UNK]']\n",
        "        # Build inverse lookup so we can go from index → token when decoding\n",
        "        self.inverse_vocabulary: dict[int, str] = {idx: tok for tok, idx in vocabulary.items()}\n",
        "\n",
        "    def __call__(self, text: str) -> list[int]:\n",
        "        \"\"\"\n",
        "        Tokenize and encode a raw string in one call.\n",
        "\n",
        "        Args:\n",
        "            text: The input string to vectorize.\n",
        "\n",
        "        Returns:\n",
        "            A list of integer token indices.\n",
        "        \"\"\"\n",
        "        # 1) clean up the text\n",
        "        clean = self.standardize(text)\n",
        "        # 2) split into character tokens\n",
        "        tokens = self.tokenize(clean)\n",
        "        # 3) map tokens → integer indices\n",
        "        return self.encode(tokens)\n",
        "\n",
        "    def standardize(self, text: str) -> str:\n",
        "        \"\"\"\n",
        "        Lowercase the text and strip out any punctuation.\n",
        "\n",
        "        Args:\n",
        "            text: Raw input string.\n",
        "\n",
        "        Returns:\n",
        "            A cleaned string containing only lowercase letters, digits, and whitespace.\n",
        "        \"\"\"\n",
        "        text = text.lower()\n",
        "        # Drop any character in string.punctuation\n",
        "        return \"\".join(ch for ch in text if ch not in string.punctuation)\n",
        "\n",
        "    def tokenize(self, text: str) -> list[str]:\n",
        "        \"\"\"\n",
        "        Split a cleaned string into a list of single-character tokens.\n",
        "\n",
        "        Args:\n",
        "            text: The output of `standardize()`.\n",
        "\n",
        "        Returns:\n",
        "            A list where each entry is one character.\n",
        "        \"\"\"\n",
        "        # The regex r'.' matches every character (including spaces)\n",
        "        return re.findall(r'.', text)\n",
        "\n",
        "    def encode(self, tokens: list[str]) -> list[int]:\n",
        "        \"\"\"\n",
        "        Map each character token to its integer index.\n",
        "\n",
        "        Args:\n",
        "            tokens: List of single-character strings.\n",
        "\n",
        "        Returns:\n",
        "            List of ints; unknown tokens become self.unk_id.\n",
        "        \"\"\"\n",
        "        return [self.vocabulary.get(tok, self.unk_id) for tok in tokens]\n",
        "\n",
        "    def decode(self, indices: list[int]) -> str:\n",
        "        \"\"\"\n",
        "        Map a sequence of indices back into a string.\n",
        "\n",
        "        Args:\n",
        "            indices: List of integer token IDs.\n",
        "\n",
        "        Returns:\n",
        "            A reconstructed string, with unknown indices as '[UNK]'.\n",
        "        \"\"\"\n",
        "        return \"\".join(self.inverse_vocabulary.get(idx, '[UNK]') for idx in indices)\n",
        "\n",
        "\n",
        "# === Quick test / sandbox for Block 2A ===\n",
        "if __name__ == \"__main__\":\n",
        "    # Build a tiny sample vocabulary\n",
        "    sample_vocab = {'[PAD]': 0, '[UNK]': 1, 'a': 2, 'b': 3, ' ': 4}\n",
        "    tok = CharTokenizer(sample_vocab)\n",
        "\n",
        "    text = \"Ab a!\"\n",
        "    print(\"Original:\", text)\n",
        "    encoded = tok(text)\n",
        "    print(\"Encoded IDs:\", encoded)           # e.g. [2, 3, 4, 2, 1]\n",
        "    print(\"Decoded back:\", tok.decode(encoded))\n",
        "    # Verify return types\n",
        "    print(\"encode() returns:\", type(encoded), \"with length\", len(encoded))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Block 2B:  Define a class for pre-processing text data at the word level."
      ],
      "metadata": {
        "id": "V1a_BYdIdRZ_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "7B23C2sEdYRL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c736b2c6-c934-42ca-c350-b489b81c0e62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original:     Hello, world!\n",
            "Standardized: hello world\n",
            "Tokens:       ['hello', 'world']\n",
            "Encoded:      [2, 3]\n",
            "Decoded:      hello world\n",
            "encode() returns: <class 'list'> length: 2\n"
          ]
        }
      ],
      "source": [
        "class WordTokenizer:\n",
        "    def __init__(self, vocabulary: dict[str, int]):\n",
        "        \"\"\"\n",
        "        Initialize a word-level tokenizer.\n",
        "\n",
        "        Args:\n",
        "            vocabulary: A dict mapping word tokens (str) to integer indices.\n",
        "                        Must include '[PAD]' and '[UNK]'.\n",
        "        \"\"\"\n",
        "        self.vocabulary: dict[str, int] = vocabulary\n",
        "        self.pad_id: int = vocabulary['[PAD]']\n",
        "        self.unk_id: int = vocabulary['[UNK]']\n",
        "        # Build reverse lookup for decode()\n",
        "        self.inverse_vocabulary: dict[int, str] = {\n",
        "            idx: tok for tok, idx in vocabulary.items()\n",
        "        }\n",
        "\n",
        "    def __call__(self, text: str) -> list[int]:\n",
        "        \"\"\"\n",
        "        Standardize → tokenize → encode in one step.\n",
        "\n",
        "        Args:\n",
        "            text: Raw input string.\n",
        "\n",
        "        Returns:\n",
        "            A list of integer token IDs.\n",
        "        \"\"\"\n",
        "        clean = self.standardize(text)\n",
        "        tokens = self.tokenize(clean)\n",
        "        return self.encode(tokens)\n",
        "\n",
        "    def standardize(self, text: str) -> str:\n",
        "        \"\"\"\n",
        "        Lowercase and strip out punctuation.\n",
        "\n",
        "        Args:\n",
        "            text: Raw input string.\n",
        "\n",
        "        Returns:\n",
        "            A cleaned string containing only lowercase letters, digits, and spaces.\n",
        "        \"\"\"\n",
        "        text = text.lower()\n",
        "        # Drop anything in string.punctuation\n",
        "        return \"\".join(ch for ch in text if ch not in string.punctuation)\n",
        "\n",
        "    def tokenize(self, text: str) -> list[str]:\n",
        "        \"\"\"\n",
        "        Split text into word-level tokens.\n",
        "\n",
        "        Args:\n",
        "            text: The standardized string.\n",
        "\n",
        "        Returns:\n",
        "            A list of tokens, where each token is one “word” plus optional punctuation.\n",
        "        \"\"\"\n",
        "        # [\\w]+  → one or more word characters (letters, digits, underscore)\n",
        "        # [.,!?;]? → optional trailing punctuation\n",
        "        return re.findall(r'[\\w]+[.,!?;]?', text)\n",
        "\n",
        "    def encode(self, tokens: list[str]) -> list[int]:\n",
        "        \"\"\"\n",
        "        Map each token to its integer index.\n",
        "\n",
        "        Args:\n",
        "            tokens: List of word tokens.\n",
        "\n",
        "        Returns:\n",
        "            List of ints; missing words become self.unk_id.\n",
        "        \"\"\"\n",
        "        return [self.vocabulary.get(tok, self.unk_id) for tok in tokens]\n",
        "\n",
        "    def decode(self, indices: list[int]) -> str:\n",
        "        \"\"\"\n",
        "        Map a sequence of indices back into a string.\n",
        "\n",
        "        Args:\n",
        "            indices: List of token IDs.\n",
        "\n",
        "        Returns:\n",
        "            A reconstructed string (words joined by spaces), with unknowns as '[UNK]'.\n",
        "        \"\"\"\n",
        "        return \" \".join(self.inverse_vocabulary.get(idx, '[UNK]') for idx in indices)\n",
        "\n",
        "\n",
        "# === Quick sandbox to verify Block 2B ===\n",
        "if __name__ == \"__main__\":\n",
        "    sample_vocab = {\n",
        "        '[PAD]': 0,\n",
        "        '[UNK]': 1,\n",
        "        'hello': 2,\n",
        "        'world': 3,\n",
        "    }\n",
        "    tok = WordTokenizer(sample_vocab)\n",
        "\n",
        "    text = \"Hello, world!\"\n",
        "    encoded = tok(text)\n",
        "\n",
        "    print(\"Original:    \", text)\n",
        "    print(\"Standardized:\", tok.standardize(text))      # \"hello world\"\n",
        "    print(\"Tokens:      \", tok.tokenize(tok.standardize(text)))  # [\"hello\", \"world\"]\n",
        "    print(\"Encoded:     \", encoded)                   # [2, 3]\n",
        "    print(\"Decoded:     \", tok.decode(encoded))       # \"hello world\"\n",
        "    print(\"encode() returns:\", type(encoded), \"length:\", len(encoded))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Block 3: Retrieve the chosen book from Project Gutenberg.  Find the line numbers of the beginning and ending lines of the actual text of the book.  These line numbers will be used to avoid including extraneous text in the analysis."
      ],
      "metadata": {
        "id": "FnbNv4qoJH1M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the book as a list of strings.  Each string is a line from\n",
        "# the text file, terminated by a newline character.\n",
        "# Block 3: Download & trim Project Gutenberg text\n",
        "\n",
        "from tensorflow.keras.utils import get_file\n",
        "\n",
        "# 1. Download and read all lines\n",
        "path = get_file(\n",
        "    'pg46.txt',\n",
        "    origin='https://www.gutenberg.org/cache/epub/46/pg46.txt'\n",
        ")\n",
        "with open(path, encoding='utf-8') as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "# 2. Find the slice points by substring (works whether the line \"starts with\" or not)\n",
        "start_idx = next(i for i, L in enumerate(lines) if 'START OF' in L) + 1\n",
        "end_idx   = next(i for i, L in enumerate(lines) if 'END OF'   in L)\n",
        "\n",
        "# 3. Extract just the book text\n",
        "book = lines[start_idx:end_idx]\n",
        "\n",
        "# 4. Sanity check & report\n",
        "assert all('START OF' not in L for L in book)\n",
        "assert all('END OF'   not in L for L in book)\n",
        "print(f\"Trimmed to lines {start_idx}–{end_idx}, total {len(book)} lines\")\n"
      ],
      "metadata": {
        "id": "JWrvBknSJHUf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a093b1c-f8c3-4d1d-fe27-bc3106fcf85d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trimmed to lines 25–3550, total 3525 lines\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Block 4:  Verify the book content."
      ],
      "metadata": {
        "id": "XhlZdpvCr53S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Add code in this cell to display short portions from the beginning and end of the book.\n",
        "\n",
        "# How many lines to show at each end\n",
        "N = 20\n",
        "\n",
        "print(f\"---- First {N} lines of the book ----\\n\")\n",
        "# Join preserves the original newlines\n",
        "print(\"\".join(book[:N]))\n",
        "\n",
        "print(f\"\\n---- Last {N} lines of the book ----\\n\")\n",
        "print(\"\".join(book[-N:]))"
      ],
      "metadata": {
        "id": "j4rKcCUxL4XK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7ec1470-2cf6-410b-a5f3-f11c7acf8ca7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---- First 20 lines of the book ----\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "A CHRISTMAS CAROL\n",
            "\n",
            "IN PROSE\n",
            "BEING\n",
            "A Ghost Story of Christmas\n",
            "\n",
            "by Charles Dickens\n",
            "\n",
            "\n",
            "\n",
            "PREFACE\n",
            "\n",
            "I HAVE endeavoured in this Ghostly little book,\n",
            "to raise the Ghost of an Idea, which shall not put my\n",
            "readers out of humour with themselves, with each other,\n",
            "with the season, or with me.  May it haunt their houses\n",
            "\n",
            "\n",
            "---- Last 20 lines of the book ----\n",
            "\n",
            "me. Assure me that I yet may change these shadows you\n",
            "have shown me, by an altered life!\"\n",
            "\n",
            "The kind hand trembled.\n",
            "\n",
            "\"I will honour Christmas in my heart, and try to keep it\n",
            "all the year. I will live in the Past, the Present, and the\n",
            "Future. The Spirits of all Three shall strive within me. I\n",
            "will not shut out the lessons that they teach. Oh, tell me I\n",
            "may sponge away the writing on this stone!\"\n",
            "\n",
            "In his agony, he caught the spectral hand. It sought to\n",
            "free itself, but he was strong in his entreaty, and detained it.\n",
            "The Spirit, stronger yet, repulsed him.\n",
            "\n",
            "Holding up his hands in a last prayer to have his fate\n",
            "reversed, he saw an alteration in the Phantom's hood and dress.\n",
            "It shrunk, collapsed, and dwindled down into a bedpost.\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Block 5A:  Build a vocabulary and tokenize it for use with the character-level tokenizer."
      ],
      "metadata": {
        "id": "e4O6tnXmI5D_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Create the vocabulary (top 100 most frequent characters) and the tokenizer\n",
        "char_vocabulary = set_charlvl_vocabulary(book, max_size=100)\n",
        "char_tokenizer  = CharTokenizer(char_vocabulary)\n",
        "\n",
        "# 2. How big is our vocab?\n",
        "vocab_size = len(char_vocabulary)\n",
        "print(f\"Character vocabulary size: {vocab_size}\")\n",
        "\n",
        "# 3. Reconstruct the ordered list of tokens from the dict\n",
        "#    (so we can see which are most vs. least common)\n",
        "tokens_by_index = [\n",
        "    token\n",
        "    for token, idx in sorted(char_vocabulary.items(), key=lambda kv: kv[1])\n",
        "]\n",
        "\n",
        "# 4. Inspect the “top” 5 (after [PAD] & [UNK]) and the bottom 5 tokens\n",
        "print(\"Top-5 tokens:\", tokens_by_index[2:7])\n",
        "print(\"Bottom-5 tokens:\", tokens_by_index[-5:])\n",
        "\n",
        "# 5. Try it out on a sample sentence\n",
        "sample = \"It was the best of times, it was the worst of times.\"\n",
        "print(\"\\nSample sentence:\", sample)\n",
        "\n",
        "#    Expect roughly len(sample) minus punctuation characters\n",
        "expected_len = len([ch for ch in sample.lower() if ch not in string.punctuation])\n",
        "print(\"Expected encoded length (no punctuation):\", expected_len)\n",
        "\n",
        "#    Actually encode\n",
        "encoded = char_tokenizer(sample)\n",
        "print(\"Actual encoded length:\", len(encoded))\n",
        "\n",
        "#    (Optional) peek at the first 20 IDs\n",
        "print(\"First 20 token IDs:\", encoded[:20])"
      ],
      "metadata": {
        "id": "0RsVBvHMLEMc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b65fa232-0aef-49a9-9092-966c73788838"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Character vocabulary size: 44\n",
            "Top-5 tokens: [' ', 'e', 't', 'o', 'a']\n",
            "Bottom-5 tokens: [')', '1', '8', '4', '3']\n",
            "\n",
            "Sample sentence: It was the best of times, it was the worst of times.\n",
            "Expected encoded length (no punctuation): 50\n",
            "Actual encoded length: 50\n",
            "First 20 token IDs: [8, 4, 2, 15, 6, 10, 2, 4, 7, 3, 2, 23, 3, 10, 4, 2, 5, 20, 2, 4]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Block 5B:  Check that encoding and decoding at the character level are (roughly) the inverse of one another."
      ],
      "metadata": {
        "id": "xF8K3oos8MRW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Add code in this cell that checks whether encoding and decoding at the\n",
        "## character level are inverses.\n",
        "\n",
        "# Sample texts to test\n",
        "samples = [\n",
        "    \"It was the best of times, it was the worst of times.\",\n",
        "    \"Hello, World!\",\n",
        "    \"Deep Learning 101?\"\n",
        "]\n",
        "\n",
        "for text in samples:\n",
        "    # 1) Standardize removes punctuation and lowercases\n",
        "    std = char_tokenizer.standardize(text)\n",
        "    # 2) Encode then immediately decode\n",
        "    encoded = char_tokenizer(text)\n",
        "    decoded = char_tokenizer.decode(encoded)\n",
        "\n",
        "    # 3) Print a side-by-side comparison\n",
        "    print(f\"Original     : {text}\")\n",
        "    print(f\"Standardized : {std!r}\")\n",
        "    print(f\"Decoded      : {decoded!r}\")\n",
        "    print(\"Match?       :\", std == decoded)\n",
        "    print(\"-\" * 60)"
      ],
      "metadata": {
        "id": "kMe8SIVirHbL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e15679df-73a6-432a-df2b-130da41c7b91"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original     : It was the best of times, it was the worst of times.\n",
            "Standardized : 'it was the best of times it was the worst of times'\n",
            "Decoded      : 'it was the best of times it was the worst of times'\n",
            "Match?       : True\n",
            "------------------------------------------------------------\n",
            "Original     : Hello, World!\n",
            "Standardized : 'hello world'\n",
            "Decoded      : 'hello world'\n",
            "Match?       : True\n",
            "------------------------------------------------------------\n",
            "Original     : Deep Learning 101?\n",
            "Standardized : 'deep learning 101'\n",
            "Decoded      : 'deep learning 1[UNK]1'\n",
            "Match?       : False\n",
            "------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Block 6A:  Build a vocabulary and tokenize it for use with the word-level tokenizer."
      ],
      "metadata": {
        "id": "3XWYcK-fm9Vr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Create the vocabulary (top 2000 most frequent words) and the tokenizer\n",
        "word_vocabulary = set_wordlvl_vocabulary(book, max_size=2000)\n",
        "word_tokenizer  = WordTokenizer(word_vocabulary)\n",
        "\n",
        "# 2. How big is our vocab?\n",
        "vocab_size = len(word_vocabulary)\n",
        "print(f\"Word vocabulary size: {vocab_size}\")\n",
        "\n",
        "# 3. Recover tokens in index order so we can inspect frequency extremes\n",
        "tokens_by_index = [\n",
        "    token\n",
        "    for token, idx in sorted(word_vocabulary.items(), key=lambda kv: kv[1])\n",
        "]\n",
        "\n",
        "# 4. Show the 5 most-common (after [PAD],[UNK]) and 5 least-common words\n",
        "print(\"Top-5 tokens:\", tokens_by_index[2:7])\n",
        "print(\"Bottom-5 tokens:\", tokens_by_index[-5:])\n",
        "\n",
        "# 5. Test on a sample sentence\n",
        "sample = \"Natural language processing is a key part of deep learning!\"\n",
        "print(\"\\nSample sentence:\", sample)\n",
        "\n",
        "# 5a. Standardize & tokenize to see expected token count\n",
        "std_tokens    = word_tokenizer.tokenize(word_tokenizer.standardize(sample))\n",
        "expected_len  = len(std_tokens)\n",
        "print(\"Standardized tokens:\", std_tokens)\n",
        "print(\"Expected token count:\", expected_len)\n",
        "\n",
        "# 5b. Actually encode via the tokenizer\n",
        "encoded = word_tokenizer(sample)\n",
        "print(\"Actual encoded length:\", len(encoded))\n",
        "print(\"First 20 token IDs:\", encoded[:20])\n",
        "\n",
        "# 6. (Optional) Decode back to words to check inversion\n",
        "decoded = word_tokenizer.decode(encoded)\n",
        "print(\"Decoded back:\", decoded)"
      ],
      "metadata": {
        "id": "rFsZI-cxm_Ya",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f48bb6c5-ff2f-474f-ff2f-ccaee083fb9f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word vocabulary size: 2000\n",
            "Top-5 tokens: ['the', 'and', 'a', 'to', 'of']\n",
            "Bottom-5 tokens: ['know;', 'offences', 'comfortable', 'hasn', 'dislike']\n",
            "\n",
            "Sample sentence: Natural language processing is a key part of deep learning!\n",
            "Standardized tokens: ['natural', 'language', 'processing', 'is', 'a', 'key', 'part', 'of', 'deep', 'learning']\n",
            "Expected token count: 10\n",
            "Actual encoded length: 10\n",
            "First 20 token IDs: [1, 1, 1, 37, 4, 1390, 374, 6, 605, 1]\n",
            "Decoded back: [UNK] [UNK] [UNK] is a key part of deep [UNK]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Block 6B:  Check that encoding and decoding at the word level are (roughly) the inverse of one another."
      ],
      "metadata": {
        "id": "1uW7hvqM81RQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Add code in this cell that checks whether encoding and decoding at the\n",
        "## word level are inverses.\n",
        "\n",
        "# Sample sentences to test\n",
        "samples = [\n",
        "    \"It was the best of times, it was the worst of times.\",\n",
        "    \"Hello, world! This is a test of the word tokenizer.\",\n",
        "    \"Deep learning transforms natural language processing.\"\n",
        "]\n",
        "\n",
        "for text in samples:\n",
        "    std_text   = word_tokenizer.standardize(text)\n",
        "    std_tokens = word_tokenizer.tokenize(std_text)\n",
        "    encoded    = word_tokenizer(text)\n",
        "    decoded    = word_tokenizer.decode(encoded)\n",
        "\n",
        "    # Build the “expected” round-trip string by re-joining the standardized tokens\n",
        "    expected_decoded = \" \".join(std_tokens)\n",
        "\n",
        "    print(f\"Original       : {text}\")\n",
        "    print(f\"Standardized   : {std_text!r}\")\n",
        "    print(f\"Std tokens     : {std_tokens}\")\n",
        "    print(f\"Decoded        : {decoded!r}\")\n",
        "    print(f\"Matches std?   : {decoded == expected_decoded}\")\n",
        "    print(\"-\" * 60)"
      ],
      "metadata": {
        "id": "LbB5nfZB9Al-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7baf403c-ec18-4d9d-df68-48008b4bc3f0"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original       : It was the best of times, it was the worst of times.\n",
            "Standardized   : 'it was the best of times it was the worst of times'\n",
            "Std tokens     : ['it', 'was', 'the', 'best', 'of', 'times', 'it', 'was', 'the', 'worst', 'of', 'times']\n",
            "Decoded        : 'it was the best of times it was the [UNK] of times'\n",
            "Matches std?   : False\n",
            "------------------------------------------------------------\n",
            "Original       : Hello, world! This is a test of the word tokenizer.\n",
            "Standardized   : 'hello world this is a test of the word tokenizer'\n",
            "Std tokens     : ['hello', 'world', 'this', 'is', 'a', 'test', 'of', 'the', 'word', 'tokenizer']\n",
            "Decoded        : '[UNK] world this is a [UNK] of the word [UNK]'\n",
            "Matches std?   : False\n",
            "------------------------------------------------------------\n",
            "Original       : Deep learning transforms natural language processing.\n",
            "Standardized   : 'deep learning transforms natural language processing'\n",
            "Std tokens     : ['deep', 'learning', 'transforms', 'natural', 'language', 'processing']\n",
            "Decoded        : 'deep [UNK] [UNK] [UNK] [UNK] [UNK]'\n",
            "Matches std?   : False\n",
            "------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Block 6B summary**  \n",
        "- When all words are in the top-2000 vocabulary, encode→decode reproduces the standardized text exactly.  \n",
        "- Any word outside that set becomes `[UNK]`, causing the round-trip check to be False.  \n",
        "- This behavior confirms the tokenizer’s intended handling of unknown words.\n",
        "\n"
      ],
      "metadata": {
        "id": "LzdMPwNVQtCi"
      }
    }
  ]
}